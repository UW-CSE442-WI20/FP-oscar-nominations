{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cleaning name_basics.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib as path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = path.Path().absolute()\n",
    "#Importing the dataframe by every 1 million tuples because the full dataset is too large for one dataframe\n",
    "try:\n",
    "n_1 = pd.read_csv(str(rel_path) + r\"\\raw_data\\name_basics.tsv\", sep='\\t', nrows = 1000000)\n",
    "n_2 = pd.read_csv(str(rel_path) + r\"\\raw_data\\name_basics.tsv\", sep='\\t',skiprows= range(1, 1000000), nrows = 1000000)\n",
    "n_3 = pd.read_csv(str(rel_path) + r\"\\raw_data\\name_basics.tsv\", sep='\\t',skiprows= range(1, 2000000), nrows = 1000000)\n",
    "n_4 = pd.read_csv(str(rel_path) + r\"\\raw_data\\name_basics.tsv\", sep='\\t',skiprows= range(1, 3000000), nrows = 1000000)\n",
    "n_5 = pd.read_csv(str(rel_path) + r\"\\raw_data\\name_basics.tsv\", sep='\\t',skiprows= range(1, 4000000), nrows = 1000000)\n",
    "n_6 = pd.read_csv(str(rel_path) + r\"\\raw_data\\name_basics.tsv\", sep='\\t',skiprows= range(1, 5000000), nrows = 1000000)\n",
    "n_7 = pd.read_csv(str(rel_path) + r\"\\raw_data\\name_basics.tsv\", sep='\\t',skiprows= range(1, 6000000), nrows = 1000000)\n",
    "n_8 = pd.read_csv(str(rel_path) + r\"\\raw_data\\name_basics.tsv\", sep='\\t',skiprows= range(1, 7000000), nrows = 1000000)\n",
    "n_9 = pd.read_csv(str(rel_path) + r\"\\raw_data\\name_basics.tsv\", sep='\\t',skiprows= range(1, 8000000), nrows = 1000000)\n",
    "n_f = pd.read_csv(str(rel_path) + r\"\\raw_data\\name_basics.tsv\", sep='\\t',skiprows= range(1, 9000000), nrows = 1000000)\n",
    "original_tuples = len(n_1) + len(n_2) + len(n_3) + len(n_4) + len(n_5) + len(n_6) + len(n_7) + len(n_8) + len(n_9) + len(n_f)\n",
    "original_size = n_1.size + n_2.size + n_3.size + n_4.size + n_5.size + n_6.size + n_7.size + n_8.size + n_9.size + n_f.size\n",
    "except:\n",
    "    print(\"You're missing the name_basics.tsv file. Download it from the following link and unzip it to the raw_data folder. Restart then kernel and rerun\")\n",
    "    print(\"https://datasets.imdbws.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Oscar-winning names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm0778942 = n_1[n_1['nconst'] == 'nm0778942']\n",
    "nm0800943 = n_1[n_1['nconst'] == 'nm0800943']\n",
    "nm0409197 = n_1[n_1['nconst'] == 'nm0409197']\n",
    "nm0402843 = n_1[n_1['nconst'] == 'nm0402843']\n",
    "nm0306890 = n_1[n_1['nconst'] == 'nm0306890']\n",
    "nm0912403 = n_1[n_1['nconst'] == 'nm0912403']\n",
    "save = [nm0778942,\n",
    "        nm0800943,\n",
    "        nm0409197,\n",
    "        nm0402843,\n",
    "        nm0306890,\n",
    "        nm0912403]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing everyone with no titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1 = n_1[n_1['knownForTitles'] != '\\\\N']\n",
    "n_2 = n_2[n_2['knownForTitles'] != '\\\\N']\n",
    "n_3 = n_3[n_3['knownForTitles'] != '\\\\N']\n",
    "n_4 = n_4[n_4['knownForTitles'] != '\\\\N']\n",
    "n_5 = n_5[n_5['knownForTitles'] != '\\\\N']\n",
    "n_6 = n_6[n_6['knownForTitles'] != '\\\\N']\n",
    "n_7 = n_7[n_7['knownForTitles'] != '\\\\N']\n",
    "n_8 = n_8[n_8['knownForTitles'] != '\\\\N']\n",
    "n_9 = n_9[n_9['knownForTitles'] != '\\\\N']\n",
    "n_f = n_f[n_f['knownForTitles'] != '\\\\N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing everyone who was never born nor died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1 = n_1[(n_1['deathYear'] != '\\\\N') | (n_1['birthYear'] != '\\\\N')]\n",
    "n_2 = n_2[(n_2['deathYear'] != '\\\\N') | (n_2['birthYear'] != '\\\\N')]\n",
    "n_3 = n_3[(n_3['deathYear'] != '\\\\N') | (n_3['birthYear'] != '\\\\N')]\n",
    "n_4 = n_4[(n_4['deathYear'] != '\\\\N') | (n_4['birthYear'] != '\\\\N')]\n",
    "n_5 = n_5[(n_5['deathYear'] != '\\\\N') | (n_5['birthYear'] != '\\\\N')]\n",
    "n_6 = n_6[(n_6['deathYear'] != '\\\\N') | (n_6['birthYear'] != '\\\\N')]\n",
    "n_7 = n_7[(n_7['deathYear'] != '\\\\N') | (n_7['birthYear'] != '\\\\N')]\n",
    "n_8 = n_8[(n_8['deathYear'] != '\\\\N') | (n_8['birthYear'] != '\\\\N')]\n",
    "n_9 = n_9[(n_9['deathYear'] != '\\\\N') | (n_9['birthYear'] != '\\\\N')]\n",
    "n_f = n_f[(n_f['deathYear'] != '\\\\N') | (n_f['birthYear'] != '\\\\N')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remerging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.concat([\n",
    "    n_1,\n",
    "    n_2,\n",
    "    n_3,\n",
    "    n_4,\n",
    "    n_5,\n",
    "    n_6,\n",
    "    n_7,\n",
    "    n_8,\n",
    "    n_9,\n",
    "    n_f\n",
    "])\n",
    "names = names.append(save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coercing \\N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names['deathYear'] = pd.to_numeric(names['deathYear'], errors = 'coerce')\n",
    "names['birthYear'] = pd.to_numeric(names['birthYear'], errors = 'coerce')\n",
    "names = names.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.to_csv(str(rel_path) + r'\\..\\src\\names_basics.csv')\n",
    "new_tuples = len(names)\n",
    "new_size = names.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tuple Count:\t9899527\n",
      "Cleansed Tuple Count:\t495484\n",
      "Tuples Reduced By: \t94.99%\n",
      "Size Reduced By: \t94.99%\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Tuple Count:\\t\"+str(original_tuples))\n",
    "print(\"Cleansed Tuple Count:\\t\"+ str(new_tuples))\n",
    "percent_tuple_reduction = (100 - round((100 * new_tuples) / original_tuples, 2))\n",
    "print(\"Tuples Reduced By: \\t\"+ str(percent_tuple_reduction)+ \"%\")\n",
    "percent_size_reduction = (100 - round((100 * new_size) / original_size, 2))\n",
    "print('Size Reduced By: \\t' + str(percent_size_reduction) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
