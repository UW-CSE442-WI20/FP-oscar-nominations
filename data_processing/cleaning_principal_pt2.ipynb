{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Principal csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib as path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = path.Path().absolute()\n",
    "#Importing the dataframe by every 1 million tuples because the full dataset is too large for one dataframe\n",
    "# try:\n",
    "p_2 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 15000000), nrows = 1000000)\n",
    "p_3 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 16000000), nrows = 1000000)\n",
    "p_4 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 17000000), nrows = 1000000)\n",
    "p_5 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 18000000), nrows = 1000000)\n",
    "p_6 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 19000000), nrows = 1000000)\n",
    "p_7 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 20000000), nrows = 1000000)\n",
    "p_8 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 21000000), nrows = 1000000)\n",
    "p_9 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 22000000), nrows = 1000000)\n",
    "p_10 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 23000000), nrows = 1000000)\n",
    "p_11 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 24000000), nrows = 1000000)\n",
    "# p_12 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 25000000), nrows = 1000000)\n",
    "# p_13 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 26000000), nrows = 1000000)\n",
    "# p_14 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 27000000), nrows = 1000000)\n",
    "# p_15 = pd.read_csv(str(rel_path) + r\"\\raw_data\\title_principals.tsv\", sep='\\t',skiprows= range(1, 28000000), nrows = 1000000)\n",
    "# original_tuples = len(p_1) + len(p_2) + len(p_3) + len(p_4) + len(p_5) + len(p_6) + len(p_7) + len(p_8) + len(p_9) + len(p_10) + len(p_12) + len(p_13)\n",
    "# original_size = p_1.size + p_2.size + p_3.size + p_4.size + p_5.size + p_6.size + p_7.size + p_8.size + p_9.size + p_10.size + p_11.size + p_12.size + p_13.size\n",
    "# except:\n",
    "#     print(\"You're massing the title_basics.tsv file. Download it from the following link and unzip it to the raw_data folder. Then restart the kernel and rerun\")\n",
    "#     print(\"https://datasets.imdbws.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [\n",
    "    p_2,\n",
    "    p_3,\n",
    "    p_4,\n",
    "    p_5,\n",
    "    p_6,\n",
    "    p_7,\n",
    "    p_8,\n",
    "    p_9,\n",
    "    p_10,\n",
    "    p_11\n",
    "#     p_12,\n",
    "#     p_13\n",
    "#     p_14,\n",
    "#     p_15\n",
    "]\n",
    "\n",
    "for df in data_list:\n",
    "    del df['characters']\n",
    "    del df['job']\n",
    "    del df['ordering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_1 = p_1[(p_1['category'] == 'cinematographer') | (p_1['category'] == 'director') | (p_1['category'] == 'actor') | (p_1['category'] == 'actress')]\n",
    "p_2 = p_2[(p_2['category'] == 'cinematographer') | (p_2['category'] == 'director') | (p_2['category'] == 'actor') | (p_2['category'] == 'actress')]\n",
    "p_3 = p_3[(p_3['category'] == 'cinematographer') | (p_3['category'] == 'director') | (p_3['category'] == 'actor') | (p_3['category'] == 'actress')]\n",
    "p_4 = p_4[(p_4['category'] == 'cinematographer') | (p_4['category'] == 'director') | (p_4['category'] == 'actor') | (p_4['category'] == 'actress')]\n",
    "p_5 = p_5[(p_5['category'] == 'cinematographer') | (p_5['category'] == 'director') | (p_5['category'] == 'actor') | (p_5['category'] == 'actress')]\n",
    "p_6 = p_6[(p_6['category'] == 'cinematographer') | (p_6['category'] == 'director') | (p_6['category'] == 'actor') | (p_6['category'] == 'actress')]\n",
    "p_7 = p_7[(p_7['category'] == 'cinematographer') | (p_7['category'] == 'director') | (p_7['category'] == 'actor') | (p_7['category'] == 'actress')]\n",
    "p_8 = p_8[(p_8['category'] == 'cinematographer') | (p_8['category'] == 'director') | (p_8['category'] == 'actor') | (p_8['category'] == 'actress')]\n",
    "p_9 = p_9[(p_9['category'] == 'cinematographer') | (p_9['category'] == 'director') | (p_9['category'] == 'actor') | (p_9['category'] == 'actress')]\n",
    "p_10 = p_10[(p_10['category'] == 'cinematographer') | (p_10['category'] == 'director') | (p_10['category'] == 'actor') | (p_10['category'] == 'actress')]\n",
    "p_11 = p_11[(p_11['category'] == 'cinematographer') | (p_11['category'] == 'director') | (p_11['category'] == 'actor') | (p_11['category'] == 'actress')]\n",
    "# p_12 = p_12[(p_12['category'] == 'cinematographer') | (p_12['category'] == 'director') | (p_12['category'] == 'actor') | (p_12['category'] == 'actress')]\n",
    "# p_13 = p_13[(p_13['category'] == 'cinematographer') | (p_13['category'] == 'director') | (p_13['category'] == 'actor') | (p_13['category'] == 'actress')]\n",
    "# p_14 = p_14[(p_14['category'] == 'cinematographer') | (p_14['category'] == 'director') | (p_14['category'] == 'actor') | (p_14['category'] == 'actress')]\n",
    "# p_15 = p_15[(p_15['category'] == 'cinematographer') | (p_15['category'] == 'director') | (p_15['category'] == 'actor') | (p_15['category'] == 'actress')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5115275"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = pd.concat([\n",
    "#     p_1,\n",
    "    p_2,\n",
    "    p_3,\n",
    "    p_4,\n",
    "    p_5,\n",
    "    p_6,\n",
    "    p_7,\n",
    "    p_8,\n",
    "    p_9,\n",
    "    p_10,\n",
    "    p_11\n",
    "#     p_12,\n",
    "#     p_13\n",
    "#     p_14,\n",
    "#     p_15\n",
    "])\n",
    "len(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.to_csv(str(rel_path) + r'\\raw_data\\cc_pt2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
